{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56bf285",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3a31ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1afeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(pairs):\n",
    "    \"\"\"\n",
    "    \"unzips\" of groups of items into separate tuples.\n",
    "    \n",
    "    Example: pairs = [(\"a\", 1), (\"b\", 2), ...] --> ((\"a\", \"b\", ...), (1, 2, ...))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pairs : Iterable[Tuple[Any, ...]]\n",
    "        An iterable of the form ((a0, b0, c0, ...), (a1, b1, c1, ...))\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Tuples[Any, ...], ...]\n",
    "       A tuple containing the \"unzipped\" contents of `pairs`; i.e. \n",
    "       ((a0, a1, ...), (b0, b1, ...), (c0, c1), ...)\n",
    "    \"\"\"\n",
    "    return tuple(zip(*pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c411fa4",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d481c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(counter):\n",
    "    \"\"\" Convert a `letter -> count` counter to a list \n",
    "    of (letter, frequency) pairs, sorted in descending order of \n",
    "    frequency.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    counter : collections.Counter\n",
    "        letter -> count\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Tuple[str, float]]\n",
    "       A list of tuples - (letter, frequency) pairs in order\n",
    "       of descending-frequency\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from collections import Counter\n",
    "    >>> letter_count = Counter({\"a\": 1, \"b\": 3})\n",
    "    >>> letter_count\n",
    "    Counter({'a': 1, 'b': 3})\n",
    "\n",
    "    >>> normalize(letter_count)\n",
    "    [('b', 0.75), ('a', 0.25)]\n",
    "    \"\"\"\n",
    "    total = sum(counter.values())\n",
    "    return [(char, cnt/total) for char, cnt in counter.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac3ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train_lm(text, n):\n",
    "    \"\"\" Train character-based n-gram language model.\n",
    "        \n",
    "    This will learn: given a sequence of n-1 characters, what the probability\n",
    "    distribution is for the n-th character in the sequence.\n",
    "\n",
    "    For example if we train on the text:\n",
    "        text = \"cacao\"\n",
    "\n",
    "    Using a n-gram size of n=3, then the following dict would be returned.\n",
    "    See that we *normalize* each of the counts for a given history\n",
    "\n",
    "        {'ac': [('a', 1.0)],\n",
    "         'ca': [('c', 0.5), ('o', 0.5)],\n",
    "         '~c': [('a', 1.0)],\n",
    "         '~~': [('c', 1.0)]}\n",
    "\n",
    "    Tildas (\"~\") are used for padding the history when necessary, so that it's \n",
    "    possible to estimate the probability of a seeing a character when there \n",
    "    aren't (n - 1) previous characters of history available.\n",
    "\n",
    "    So, according to this text we trained on, if you see the sequence 'ac',\n",
    "    our model predicts that the next character should be 'a' 100% of the time.\n",
    "\n",
    "    For generating the padding, recall that Python allows you to generate \n",
    "    repeated sequences easily: \n",
    "       `\"p\" * 4` returns `\"pppp\"`\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    text: str \n",
    "        A string (doesn't need to be lowercased).\n",
    "        \n",
    "    n: int\n",
    "        The length of n-gram to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[Tuple[str, float]]]\n",
    "        \n",
    "        {n-1 history -> [(letter, normalized count), ...]}\n",
    "        \n",
    "        A dictionary that maps histories (strings of length (n-1)) to lists of (char, prob) \n",
    "        pairs, where prob is the probability (i.e frequency) of char appearing after \n",
    "        that specific history.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> train_lm(\"cacao\", 3)\n",
    "    {'ac': [('a', 1.0)],\n",
    "     'ca': [('c', 0.5), ('o', 0.5)],\n",
    "     '~c': [('a', 1.0)],\n",
    "     '~~': [('c', 1.0)]}\n",
    "    \"\"\"\n",
    "    raw_lm = defaultdict(Counter)\n",
    "    history = \"~\"\n",
    "    \n",
    "    # count number of times characters appear following different histories\n",
    "    # `raw_lm`: {history -> Counter}\n",
    "    for char in text:\n",
    "        h = \"\".join(history)\n",
    "        raw_lm[h][char] += 1\n",
    "        # slide history window to the right by one character\n",
    "        if n == 1:\n",
    "            history = char\n",
    "        else:\n",
    "            history = history[1:] + char\n",
    "    \n",
    "    # create final dictionary, normalizing the counts for each history\n",
    "    lm = {history : normalize(counter) for history, counter in raw_lm.items()}\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d86f3",
   "metadata": {},
   "source": [
    "# Creating text string of all lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3303d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "\n",
    "def tokenize_without_stopwords(lyrics):\n",
    "    tokenized = punc_regex.sub('', lyrics).lower().split()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d603cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd55e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e90ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get negative files: 0.03023076057434082\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "path = 'Negative Song Lyrics/*.txt'\n",
    "files = glob.glob(path, recursive=True)\n",
    "\n",
    "for file in files:\n",
    "    f = open(file, \"r\")\n",
    "    chorus = f.read().replace(\"\\n\", \" \")\n",
    "    text += tokenize_without_stopwords(chorus)\n",
    "    \n",
    "    f.close()\n",
    "t1 = time.time()\n",
    "\n",
    "# should take 5-7 seconds\n",
    "print(\"Time to get negative files:\", (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf6de1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get positive files: 0.04401350021362305\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "path = 'Positive Song Lyrics/*.txt'\n",
    "files = glob.glob(path, recursive=True)\n",
    "\n",
    "for file in files:\n",
    "    f = open(file, \"r\")\n",
    "    chorus = f.read().replace(\"\\n\", \" \")\n",
    "    text += tokenize_without_stopwords(chorus)\n",
    "    \n",
    "    f.close()\n",
    "t1 = time.time()\n",
    "\n",
    "# should take 5-7 seconds\n",
    "print(\"Time to get positive files:\", (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0d490511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_letter(lm, history):\n",
    "    \"\"\" Randomly picks letter according to probability distribution associated with \n",
    "    the specified history, as stored in your language model.\n",
    "\n",
    "    Note: returns dummy character \"~\" if history not found in model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lm: Dict[str, List[Tuple[str, float]]] \n",
    "        The n-gram language model. \n",
    "        I.e. the dictionary: history -> [(char, freq), ...]\n",
    "\n",
    "    history: str\n",
    "        A string of length (n-1) to use as context/history for generating \n",
    "        the next character.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The predicted character. '~' if history is not in language model.\n",
    "    \"\"\"\n",
    "    if not history in lm:\n",
    "        return \"~\"\n",
    "    letters, probs = unzip(lm[history])\n",
    "    i = np.random.choice(letters, p=probs)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4c58eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, n, nletters=100):\n",
    "    \"\"\" Randomly generates `nletters` of text by drawing from \n",
    "    the probability distributions stored in a n-gram language model \n",
    "    `lm`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lm: Dict[str, List[Tuple[str, float]]]\n",
    "        The n-gram language model. \n",
    "        I.e. the dictionary: history -> [(char, freq), ...]\n",
    "    \n",
    "    n: int\n",
    "        Order of n-gram model.\n",
    "    \n",
    "    nletters: int\n",
    "        Number of letters to randomly generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Model-generated text. Should contain `nletters` number of\n",
    "        generated characters. The pre-pended ~'s are not to be included. \n",
    "    \"\"\"\n",
    "    history = \"~\"\n",
    "    text = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history)\n",
    "        #print(\"c\", c)\n",
    "        text.append(c)\n",
    "        if n == 1:\n",
    "            history = c\n",
    "        else:\n",
    "            history = history[1:] + c\n",
    "    return \" \".join(text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c8d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(lm, 1, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
